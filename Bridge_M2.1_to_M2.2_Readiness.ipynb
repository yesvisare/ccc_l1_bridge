{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge M2.1 → M2.2: Readiness Validation\n",
    "\n",
    "**Purpose:** Validate completion of M2.1 Caching Strategies before proceeding to M2.2 Prompt Optimization.\n",
    "\n",
    "**Duration:** 5-10 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVED_SECTION:1 - Recap: M2.1 Accomplishments\n",
    "\n",
    "### What You Built in M2.1\n",
    "\n",
    "✓ **Multi-Layer Redis Cache System**  \n",
    "Four layers (response, semantic, embedding, context) that reduce RAG costs by 30-70%.\n",
    "\n",
    "✓ **Cache Invalidation Strategy**  \n",
    "TTL, event-based, and LRU policies balancing freshness vs. cost savings.\n",
    "\n",
    "✓ **Debugged 5 Production Failures**\n",
    "1. Cache stampede on cold start\n",
    "2. Stale data after updates\n",
    "3. Redis memory overflow (OOM)\n",
    "4. Hash collisions\n",
    "5. Connection timeouts\n",
    "\n",
    "✓ **Query Diversity Analysis**  \n",
    "Learned when caching works (similarity >30%) and when to skip it (>90% diversity).\n",
    "\n",
    "---\n",
    "\n",
    "### The Cache Miss Problem\n",
    "\n",
    "**Your cache hit rate:** 40% (typical)  \n",
    "**Cache miss rate:** 60% of queries still hit full pipeline\n",
    "\n",
    "**Cost calculation:**\n",
    "- 10,000 queries/day\n",
    "- 6,000 cache misses/day\n",
    "- Cost per miss: $0.0021\n",
    "- **Daily cost:** $12.60 = **$378/month**\n",
    "\n",
    "**M2.2 Goal:** Reduce cache miss costs by 30-50% through prompt optimization.\n",
    "\n",
    "**Combined savings:** 40% (caching) + 30-50% (prompt optimization) = **50-70% total cost reduction**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## SAVED_SECTION:2 - Check #1: Cache Hit Rate\n\n**Requirement:** Redis cache operational with >30% hit rate\n\n**Why:** Low hit rate (<30%) means caching overhead exceeds savings; prompt optimization becomes critical.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Expected: cache_hit_rate >= 0.30\n# Options: (1) Parse redis-cli INFO stats, (2) Manual input, (3) Stub JSON\n\nimport json\nimport os\n\n# Option 1: Try parsing Redis stats (if available)\ndef get_redis_hit_rate():\n    try:\n        import subprocess\n        result = subprocess.run(['redis-cli', 'INFO', 'stats'], \n                              capture_output=True, text=True, timeout=2)\n        if result.returncode == 0:\n            lines = result.stdout.split('\\n')\n            hits = misses = 0\n            for line in lines:\n                if 'keyspace_hits:' in line:\n                    hits = int(line.split(':')[1])\n                if 'keyspace_misses:' in line:\n                    misses = int(line.split(':')[1])\n            if hits + misses > 0:\n                return hits / (hits + misses)\n    except Exception as e:\n        print(f\"Redis not available: {e}\")\n    return None\n\n# Option 2: Manual input fallback\ndef manual_input_hit_rate():\n    print(\"Enter your cache hit rate (0.0-1.0) or press Enter to use stub:\")\n    user_input = input().strip()\n    if user_input:\n        return float(user_input)\n    return None\n\n# Option 3: Stub from file\ndef stub_hit_rate():\n    stub_file = 'cache_metrics_stub.json'\n    if os.path.exists(stub_file):\n        with open(stub_file) as f:\n            data = json.load(f)\n            return data.get('cache_hit_rate', 0.40)\n    # Create stub if missing\n    stub_data = {'cache_hit_rate': 0.40, 'note': 'Replace with actual metrics'}\n    with open(stub_file, 'w') as f:\n        json.dump(stub_data, f, indent=2)\n    print(f\"Created {stub_file} with default hit rate 0.40\")\n    return 0.40\n\n# Try methods in order\nhit_rate = get_redis_hit_rate()\nif hit_rate is None:\n    hit_rate = manual_input_hit_rate()\nif hit_rate is None:\n    hit_rate = stub_hit_rate()\n\n# Validate\nprint(f\"\\n✓ Cache Hit Rate: {hit_rate:.1%}\")\nif hit_rate >= 0.30:\n    print(\"✓ PASS: Hit rate meets minimum threshold (>=30%)\")\nelse:\n    print(f\"⚠ WARNING: Hit rate {hit_rate:.1%} < 30%\")\n    print(\"  → Return to M2.1 Augmented [14:30] for semantic cache tuning\")\n    print(\"  → Prompt optimization will carry more load in M2.2\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Expected: cost_savings_vs_baseline field exists showing 30-70% reduction\n\nimport json\nimport os\n\nstub_file = 'cost_analytics_stub.json'\n\n# Check if analytics file exists\nif os.path.exists(stub_file):\n    with open(stub_file) as f:\n        analytics = json.load(f)\n    print(f\"✓ Found {stub_file}\")\nelse:\n    # Create stub template\n    analytics = {\n        \"baseline_cost_per_day\": 21.00,\n        \"current_cost_per_day\": 12.60,\n        \"cost_savings_vs_baseline\": 0.40,\n        \"note\": \"Replace with actual analytics data\"\n    }\n    with open(stub_file, 'w') as f:\n        json.dump(analytics, f, indent=2)\n    print(f\"Created {stub_file} with template data\")\n\n# Validate required field\nif 'cost_savings_vs_baseline' in analytics:\n    savings = analytics['cost_savings_vs_baseline']\n    baseline = analytics.get('baseline_cost_per_day', 0)\n    current = analytics.get('current_cost_per_day', 0)\n    \n    print(f\"\\n✓ Baseline cost/day: ${baseline:.2f}\")\n    print(f\"✓ Current cost/day: ${current:.2f}\")\n    print(f\"✓ Cost savings: {savings:.1%}\")\n    \n    if 0.30 <= savings <= 0.70:\n        print(\"✓ PASS: Savings within expected range (30-70%)\")\n    else:\n        print(f\"⚠ NOTE: Savings {savings:.1%} outside typical 30-70% range\")\nelse:\n    print(\"✗ FAIL: 'cost_savings_vs_baseline' field missing\")\n    print(f\"  → Add this field to {stub_file}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## SAVED_SECTION:3 - Check #2: Cost Savings Baseline\n\n**Requirement:** Analytics showing `cost_savings_vs_baseline` with 30-40% reduction\n\n**Why:** Baseline metrics let you measure compound savings when adding prompt optimization; no baseline = can't prove ROI.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## SAVED_SECTION:6 - Call-Forward: M2.2 Preview\n\n### What's Next in M2.2: Prompt Optimization & Model Selection\n\nYou'll add three capabilities to reduce cache miss costs by 30-50%:\n\n---\n\n#### 1. RAG-Specific Prompt Engineering\n**Goal:** Reduce token usage 30-50% with 7 optimization templates  \n**Trade-off:** Trading context/verbosity for cost savings  \n**Technique:** A/B test templates to find quality-cost sweet spot\n\n#### 2. Intelligent Model Routing\n**Goal:** Route simple → cheap models (GPT-3.5), complex → premium (GPT-4)  \n**Trade-off:** Balancing cost vs capability  \n**Risk:** Wrong routing = overpaying or poor answers\n\n#### 3. Token Optimization Techniques\n**Goal:** Smart truncation, compression, summarization to stay under token limits  \n**Trade-off:** Risking context loss if too aggressive  \n**Challenge:** When is 140 tokens enough vs. needing full 350?\n\n---\n\n### Critical Heads-Up\n\n**Aggressive prompt optimization can degrade answer quality.**\n\nWhen you cut a prompt from 350 → 140 tokens, you're removing context and nuance.\n\n**The sweet spot:** 30-40% token reduction with <5% quality degradation.\n\nM2.2 teaches you to optimize without breaking user trust—measuring quality alongside cost.\n\n---\n\n### The Question for M2.2\n\n**\"How do you optimize prompts to cut costs 30-50% without degrading answer quality?\"**\n\n**Technical preview:**\n- Use `tiktoken` for token counting\n- Implement template-based optimization with A/B testing\n- Build model router using complexity scoring\n\n**Combined result:** 50-70% total cost reduction (caching + prompt optimization)\n\n---\n\n**Estimated time:** 40 min video + 60-90 min hands-on\n\n**Ready for M2.2!** ✓\n\n---\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Expected: CSV or JSON with query diversity distribution\n\nimport json\nimport os\nimport csv\n\n# Check for diversity metrics files\nmetric_files = ['query_diversity.json', 'query_diversity.csv', 'diversity_metrics.json']\nstub_file = 'query_diversity.json'\n\nfound = False\nfor f in metric_files:\n    if os.path.exists(f):\n        print(f\"✓ Found: {f}\")\n        \n        if f.endswith('.json'):\n            with open(f) as file:\n                data = json.load(file)\n                diversity = data.get('diversity_score', data.get('average_diversity', 0))\n                print(f\"\\n✓ Diversity score: {diversity:.1%}\")\n        elif f.endswith('.csv'):\n            with open(f) as file:\n                reader = csv.DictReader(file)\n                rows = list(reader)\n                print(f\"\\n✓ Found {len(rows)} diversity records\")\n        \n        found = True\n        break\n\nif not found:\n    # Create stub\n    print(f\"No diversity metrics found. Creating {stub_file} template...\")\n    \n    stub_data = {\n        \"diversity_score\": 0.45,\n        \"total_queries\": 1000,\n        \"unique_patterns\": 450,\n        \"similarity_distribution\": {\n            \"0-30%\": 0.20,\n            \"30-70%\": 0.50,\n            \"70-100%\": 0.30\n        },\n        \"note\": \"Replace with actual query diversity analysis\"\n    }\n    \n    with open(stub_file, 'w') as f:\n        json.dump(stub_data, f, indent=2)\n    \n    print(f\"✓ Created {stub_file} with template data\")\n    diversity = stub_data['diversity_score']\n\n# Strategy recommendation\nif found or diversity:\n    print(\"\\n--- M2.2 Strategy Recommendation ---\")\n    if diversity > 0.70:\n        print(\"⚠ High diversity (>70%)\")\n        print(\"  → Focus: Prompt compression in M2.2\")\n        print(\"  → Reason: Low cache hit potential, optimize each query\")\n    elif diversity < 0.30:\n        print(\"✓ Low diversity (<30%)\")\n        print(\"  → Focus: Semantic cache tuning\")\n        print(\"  → Reason: High cache hit potential, maximize cache efficiency\")\n    else:\n        print(\"✓ Medium diversity (30-70%)\")\n        print(\"  → Focus: Balanced approach (caching + prompt optimization)\")\n        print(\"  → Reason: Both strategies contribute significantly\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## SAVED_SECTION:5 - Check #4: Query Diversity Metric\n\n**Requirement:** Query diversity calculated from logs (CSV/JSON)\n\n**Why:** Determines M2.2 optimization strategy:\n- High diversity (>70%): Focus on prompt compression\n- Low diversity (<30%): Focus on semantic cache tuning\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Expected: File or README documenting all 5 failures + fixes\n\nimport os\n\n# Check for documentation files\ndoc_files = ['failures_documentation.md', 'README.md', 'FAILURES.md']\nstub_file = 'failures_documentation.md'\n\nrequired_failures = [\n    'cache stampede',\n    'stale data',\n    'memory overflow',\n    'hash collision',\n    'connection timeout'\n]\n\nfound_file = None\nfor f in doc_files:\n    if os.path.exists(f):\n        with open(f, 'r') as file:\n            content = file.read().lower()\n            found_file = f\n            break\n\nif found_file:\n    print(f\"✓ Found documentation: {found_file}\")\n    \n    # Check for all 5 failures\n    found_count = sum(1 for failure in required_failures if failure in content)\n    \n    print(f\"\\n✓ Failures documented: {found_count}/5\")\n    for failure in required_failures:\n        status = \"✓\" if failure in content else \"✗\"\n        print(f\"  {status} {failure.title()}\")\n    \n    if found_count >= 5:\n        print(\"\\n✓ PASS: All 5 failures documented\")\n    else:\n        print(f\"\\n⚠ NOTE: Only {found_count}/5 failures found in documentation\")\nelse:\n    # Create stub template\n    print(f\"No documentation found. Creating {stub_file} template...\")\n    \n    template = \"\"\"# Cache Failures & Fixes\n\n## 1. Cache Stampede on Cold Start\n**Problem:** [Describe the issue]\n**Fix:** [Your solution]\n\n## 2. Stale Data After Updates\n**Problem:** [Describe the issue]\n**Fix:** [Your solution]\n\n## 3. Redis Memory Overflow (OOM)\n**Problem:** [Describe the issue]\n**Fix:** [Your solution]\n\n## 4. Hash Collisions\n**Problem:** [Describe the issue]\n**Fix:** [Your solution]\n\n## 5. Connection Timeouts\n**Problem:** [Describe the issue]\n**Fix:** [Your solution]\n\"\"\"\n    \n    with open(stub_file, 'w') as f:\n        f.write(template)\n    \n    print(f\"✓ Created {stub_file} template\")\n    print(f\"⚠ ACTION REQUIRED: Fill in problem descriptions and fixes\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## SAVED_SECTION:4 - Check #3: Failures Documented\n\n**Requirement:** All 5 common failures documented with fixes\n\n**Why:** Prevents 4+ hours debugging same issues in M2.2 when caching interacts with optimized prompts; portfolio evidence.\n\n**The 5 Failures:**\n1. Cache stampede on cold start\n2. Stale data after updates\n3. Redis memory overflow (OOM)\n4. Hash collisions\n5. Connection timeouts\n\n---",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}