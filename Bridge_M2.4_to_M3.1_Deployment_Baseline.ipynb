{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge M2.4 â†’ M3.1: Deployment-Gap Validation Baseline\n",
    "\n",
    "**Purpose:** Capture baseline deployment metrics and identify gaps before entering Module 3: Production Deployment\n",
    "\n",
    "**Context:** Module 2 Complete (Cost Optimization, Monitoring, Reliability) â†’ Module 3 (Production Deployment)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Module 2 Recap - What We Shipped\n",
    "\n",
    "### âœ… M2.1: Caching Strategies for Cost Reduction\n",
    "- **Shipped:** 3-layer semantic cache cutting costs 30-70%\n",
    "- **Shipped:** TTL-based cache invalidation preventing stale data\n",
    "- **Shipped:** Cache hit rate measurement and optimization\n",
    "- **Trade-off understood:** Freshness vs cost\n",
    "\n",
    "### âœ… M2.2: Prompt Optimization & Model Selection\n",
    "- **Shipped:** Prompt size reduction by 40-60% through structured formatting\n",
    "- **Shipped:** Intelligent routing (GPT-3.5 vs GPT-4) saving 30-50% on API costs\n",
    "- **Shipped:** Quality vs cost measurement framework\n",
    "- **Trade-off understood:** Token optimization can degrade answer quality\n",
    "\n",
    "### âœ… M2.3: Production Monitoring Dashboard\n",
    "- **Shipped:** Prometheus + Grafana for metrics collection and visualization\n",
    "- **Shipped:** Structured logging with trace IDs for debugging\n",
    "- **Shipped:** 8 critical alerts (errors, latency, cost, cache performance)\n",
    "- **Shipped:** Real-time system health dashboards\n",
    "\n",
    "### âœ… M2.4: Error Handling & Reliability\n",
    "- **Shipped:** Retry logic with exponential backoff + jitter\n",
    "- **Shipped:** Circuit breaker isolating Pinecone failures\n",
    "- **Shipped:** 3-level graceful degradation (FULL/DEGRADED/MINIMAL)\n",
    "- **Shipped:** 5 failure scenario tests with validated recovery\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Portfolio-Worthy Achievement\n",
    "\n",
    "**What you built:**\n",
    "- Cost-optimized RAG system running at 40-70% lower expense\n",
    "- Full observability with metrics, logs, and alerts\n",
    "- Resilience patterns handling 95%+ of errors automatically\n",
    "- Production-grade monitoring identifying issues before users notice\n",
    "\n",
    "**Typical Results:**\n",
    "- **Before M2:** $300-600/month API costs, 5-10% error rate, 4-6 hours/week debugging\n",
    "- **After M2:** $90-180/month API costs (70% reduction), 95% error recovery, 2-3 hours/week saved\n",
    "- **ROI:** $210-420/month saved Ã— 12 = $2,520-5,040/year\n",
    "\n",
    "---\n",
    "\n",
    "**SAVED_SECTION: 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Section 2: Current Deployment Process - Manual Steps Baseline\n\n**The Deployment Gap:** Your RAG system is optimized, monitored, and resilient... but it's running on localhost:8000.\n\n### Problems with Current State:\n- âŒ Can't serve real users from your laptop\n- âŒ No public URL accessible from anywhere\n- âŒ No HTTPS with valid SSL certificate\n- âŒ No automatic restarts if process crashes\n- âŒ No environment-based configuration (dev vs prod)\n- âŒ System disappears when laptop closes or internet goes down\n\n### Cost of Manual Deployment:\n- **Time cost:** 30-60 minutes per deployment Ã— 5 deployments/week = 2.5-5 hours/week\n- **Risk cost:** 2 hour recovery time Ã— 3 incidents/month Ã— 500 users = 3,000 user-hours of downtime/month\n- **Opportunity cost:** 10-15 hours/month on operational toil instead of building features\n- **Total:** $500-750/month in lost productivity at $50/hour opportunity cost\n\n---\n\n### ğŸ“ Exercise: Document Your Manual Deployment Steps\n\n**Instructions:** Fill in the JSON file below with your current manual deployment process. Run the cell to save to `deployment_process.json`.\n\n**Expected:** 8-12 manual steps that you currently perform to deploy/run your RAG system.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nfrom datetime import datetime\n\n# Template: Fill in your actual deployment steps\ndeployment_process = {\n    \"captured_at\": datetime.now().isoformat(),\n    \"current_manual_steps\": [\n        {\"step\": 1, \"action\": \"Check .env file for API keys and configuration\", \"estimated_time_minutes\": 2},\n        {\"step\": 2, \"action\": \"Start Redis cache server locally\", \"estimated_time_minutes\": 1},\n        {\"step\": 3, \"action\": \"Start ChromaDB vector database\", \"estimated_time_minutes\": 2},\n        {\"step\": 4, \"action\": \"Start Prometheus for metrics collection\", \"estimated_time_minutes\": 1},\n        {\"step\": 5, \"action\": \"Start Grafana for dashboard visualization\", \"estimated_time_minutes\": 1},\n        {\"step\": 6, \"action\": \"Run database migrations if schema changed\", \"estimated_time_minutes\": 3},\n        {\"step\": 7, \"action\": \"Start FastAPI application server\", \"estimated_time_minutes\": 2},\n        {\"step\": 8, \"action\": \"Check logs for startup errors\", \"estimated_time_minutes\": 3},\n        {\"step\": 9, \"action\": \"Test health endpoint (curl localhost:8000/health)\", \"estimated_time_minutes\": 1},\n        {\"step\": 10, \"action\": \"Test RAG query endpoint with sample query\", \"estimated_time_minutes\": 2},\n        {\"step\": 11, \"action\": \"Check Grafana dashboards for metric collection\", \"estimated_time_minutes\": 2},\n        {\"step\": 12, \"action\": \"Monitor initial requests for errors\", \"estimated_time_minutes\": 5}\n    ],\n    \"total_estimated_time_minutes\": 25,\n    \"deployment_frequency_per_week\": 5,\n    \"weekly_time_cost_hours\": 2.1\n}\n\n# Save to JSON file\nwith open(\"deployment_process.json\", \"w\") as f:\n    json.dump(deployment_process, f, indent=2)\n\nprint(\"âœ… Deployment process captured!\")\nprint(f\"ğŸ“Š Total manual steps: {len(deployment_process['current_manual_steps'])}\")\nprint(f\"â±ï¸  Estimated time per deployment: {deployment_process['total_estimated_time_minutes']} minutes\")\nprint(f\"ğŸ“… Weekly time cost: {deployment_process['weekly_time_cost_hours']} hours\")\nprint(\"\\nğŸ’¡ Edit the steps above to match YOUR actual deployment process, then re-run this cell.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n**SAVED_SECTION: 2** - Current deployment process captured to `deployment_process.json`",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Section 3: MTTR Baseline - Mean Time To Recovery\n\n**Purpose:** Measure how long it takes to detect and recover from a production incident in your current setup.\n\n### What is MTTR?\n\n**Mean Time To Recovery (MTTR)** = Time from failure occurrence â†’ Service fully restored\n\n**Formula:** MTTR = Detection Time + Diagnosis Time + Fix Time + Verification Time\n\n---\n\n### Why MTTR Matters:\n\n**Production incident scenario:**\n- Your RAG system crashes at 2 AM\n- Without automated monitoring, you don't know until users complain at 9 AM\n- **Detection time:** 7 hours\n- You investigate logs manually: **Diagnosis time:** 30 minutes\n- You restart services and redeploy: **Fix time:** 15 minutes\n- You verify with test queries: **Verification time:** 10 minutes\n- **Total MTTR:** 7 hours 55 minutes\n\n**Impact:** 500 users Ã— 7.9 hours = 3,950 user-hours of downtime from ONE incident\n\n---\n\n### ğŸ§ª Exercise: Simulate Failure & Measure MTTR\n\n**Instructions:**\n1. With your RAG system running, simulate a failure (kill a process, disconnect database, etc.)\n2. Time how long it takes you to:\n   - **Detect** the failure (check monitoring, logs, alerts)\n   - **Diagnose** the root cause (which component failed?)\n   - **Fix** the issue (restart service, fix config, etc.)\n   - **Verify** recovery (test endpoints, check metrics)\n3. Record your times in the cell below\n\n**Note:** If you can't run the simulation now, you can manually enter estimated times based on past incidents.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nfrom datetime import datetime\n\n# Record your MTTR measurements here\n# Edit these values based on your actual or estimated times\nmttr_baseline = {\n    \"captured_at\": datetime.now().isoformat(),\n    \"simulation_type\": \"Manual entry (update this: 'Actual simulation' or 'Estimated from past incidents')\",\n    \"failure_scenario\": \"FastAPI process crash (update with your actual scenario)\",\n    \n    # MTTR Components (in minutes)\n    \"detection_time_minutes\": 30,  # How long until you noticed the failure?\n    \"diagnosis_time_minutes\": 15,  # How long to identify the root cause?\n    \"fix_time_minutes\": 10,        # How long to fix and redeploy?\n    \"verification_time_minutes\": 5, # How long to verify recovery?\n    \n    # Calculated totals\n    \"total_mttr_minutes\": 60,\n    \"total_mttr_hours\": 1.0,\n    \n    # Context\n    \"detection_method\": \"Manual check (update: 'Alert', 'User complaint', 'Manual check', 'Scheduled health check')\",\n    \"monitoring_available\": True,  # Do you have Prometheus/Grafana monitoring?\n    \"alerts_configured\": False,    # Do you have alerts for failures?\n    \"automatic_recovery\": False,   # Is there any automatic restart mechanism?\n    \n    # Impact calculation\n    \"estimated_users_affected\": 100,\n    \"user_hours_lost\": 100,  # users Ã— MTTR hours\n    \"incidents_per_month\": 3,\n    \"monthly_downtime_hours\": 3.0,\n    \"monthly_user_hours_lost\": 300\n}\n\n# Calculate totals\nmttr_baseline[\"total_mttr_minutes\"] = (\n    mttr_baseline[\"detection_time_minutes\"] +\n    mttr_baseline[\"diagnosis_time_minutes\"] +\n    mttr_baseline[\"fix_time_minutes\"] +\n    mttr_baseline[\"verification_time_minutes\"]\n)\nmttr_baseline[\"total_mttr_hours\"] = round(mttr_baseline[\"total_mttr_minutes\"] / 60, 2)\nmttr_baseline[\"user_hours_lost\"] = mttr_baseline[\"estimated_users_affected\"] * mttr_baseline[\"total_mttr_hours\"]\nmttr_baseline[\"monthly_downtime_hours\"] = mttr_baseline[\"total_mttr_hours\"] * mttr_baseline[\"incidents_per_month\"]\nmttr_baseline[\"monthly_user_hours_lost\"] = mttr_baseline[\"user_hours_lost\"] * mttr_baseline[\"incidents_per_month\"]\n\n# Save to JSON file\nwith open(\"mttr_baseline.json\", \"w\") as f:\n    json.dump(mttr_baseline, f, indent=2)\n\nprint(\"âœ… MTTR baseline captured!\")\nprint(f\"\\nğŸ“Š MTTR Breakdown:\")\nprint(f\"  ğŸ” Detection: {mttr_baseline['detection_time_minutes']} min\")\nprint(f\"  ğŸ”¬ Diagnosis: {mttr_baseline['diagnosis_time_minutes']} min\")\nprint(f\"  ğŸ”§ Fix: {mttr_baseline['fix_time_minutes']} min\")\nprint(f\"  âœ“ Verification: {mttr_baseline['verification_time_minutes']} min\")\nprint(f\"  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\nprint(f\"  â±ï¸  Total MTTR: {mttr_baseline['total_mttr_minutes']} min ({mttr_baseline['total_mttr_hours']} hours)\")\nprint(f\"\\nğŸ“ˆ Impact:\")\nprint(f\"  ğŸ‘¥ Users affected per incident: {mttr_baseline['estimated_users_affected']}\")\nprint(f\"  â° User-hours lost per incident: {mttr_baseline['user_hours_lost']}\")\nprint(f\"  ğŸ“… Estimated incidents/month: {mttr_baseline['incidents_per_month']}\")\nprint(f\"  ğŸš¨ Monthly downtime: {mttr_baseline['monthly_downtime_hours']} hours\")\nprint(f\"  ğŸ’¥ Monthly user-hours lost: {mttr_baseline['monthly_user_hours_lost']}\")\nprint(f\"\\nğŸ¯ Module 3 Goal: Reduce MTTR to <5 minutes with automated health checks and restarts\")\nprint(\"\\nğŸ’¡ Edit the values above to match YOUR actual measurements, then re-run this cell.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n**SAVED_SECTION: 3** - MTTR baseline captured to `mttr_baseline.json`",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Section 4: Top 3 Deployment Risks\n\n**Purpose:** Identify the biggest risks to successful deployment and plan mitigation strategies.\n\n### Common Deployment Risk Categories:\n\n1. **Configuration Risks:** Missing environment variables, incorrect API keys, database connection strings\n2. **Dependency Risks:** Version conflicts, missing packages, incompatible system libraries\n3. **Infrastructure Risks:** Insufficient memory/CPU, network connectivity, port conflicts\n4. **Data Risks:** Missing vector database data, cache not initialized, migration failures\n5. **Security Risks:** Exposed secrets, unsecured endpoints, CORS misconfiguration\n6. **Monitoring Gaps:** No health checks, missing metrics, silent failures\n\n---\n\n### ğŸš¨ Exercise: Identify Your Top 3 Deployment Risks\n\n**Instructions:** Think about what could go wrong when deploying your RAG system to production. Fill in the risk data below and run the cell to save to `deployment_risks.json`.\n\n**Expected:** 3 specific risks ranked by likelihood Ã— impact severity.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nfrom datetime import datetime\n\n# Document your top 3 deployment risks\n# Edit these based on YOUR system's specific risks\ndeployment_risks = {\n    \"captured_at\": datetime.now().isoformat(),\n    \"risks\": [\n        {\n            \"rank\": 1,\n            \"title\": \"Missing or incorrect environment variables in production\",\n            \"category\": \"Configuration\",\n            \"description\": \"API keys, database URLs, or service credentials not properly configured in cloud environment\",\n            \"likelihood\": \"High\",  # High/Medium/Low\n            \"impact\": \"Critical\",  # Critical/High/Medium/Low\n            \"current_mitigation\": \"None - manual .env file management\",\n            \"m3_mitigation\": \"Environment variable validation at startup + .env.example template + Railway/Render secrets management\"\n        },\n        {\n            \"rank\": 2,\n            \"title\": \"Vector database (Chroma/Pinecone) connection failure in production\",\n            \"category\": \"Infrastructure\",\n            \"description\": \"Database not accessible from cloud environment due to network restrictions or missing data initialization\",\n            \"likelihood\": \"Medium\",\n            \"impact\": \"Critical\",\n            \"current_mitigation\": \"Circuit breaker pattern implemented in M2.4\",\n            \"m3_mitigation\": \"Docker container networking + health checks + persistent volume mounting for ChromaDB\"\n        },\n        {\n            \"rank\": 3,\n            \"title\": \"Insufficient memory causing container restarts under load\",\n            \"category\": \"Infrastructure\",\n            \"description\": \"RAG system with embedding models + cache + vector DB requires significant memory (2-4GB). Cloud free tiers may be insufficient.\",\n            \"likelihood\": \"Medium\",\n            \"impact\": \"High\",\n            \"current_mitigation\": \"None - not tested under production load\",\n            \"m3_mitigation\": \"Load testing in M3.4 + right-sizing cloud instance + memory monitoring alerts\"\n        }\n    ],\n    \"risk_matrix\": {\n        \"critical_risks\": 1,  # Likelihood High + Impact Critical\n        \"high_risks\": 1,      # Likelihood Medium + Impact Critical/High\n        \"medium_risks\": 1,    # Likelihood Medium + Impact Medium\n        \"low_risks\": 0\n    },\n    \"overall_deployment_readiness\": \"MEDIUM - Multiple high-impact risks identified, mitigations planned for M3\"\n}\n\n# Calculate risk counts\ncritical = sum(1 for r in deployment_risks[\"risks\"] if r[\"likelihood\"] == \"High\" and r[\"impact\"] == \"Critical\")\nhigh = sum(1 for r in deployment_risks[\"risks\"] if (r[\"likelihood\"] == \"Medium\" and r[\"impact\"] in [\"Critical\", \"High\"]) or (r[\"likelihood\"] == \"High\" and r[\"impact\"] == \"High\"))\nmedium = sum(1 for r in deployment_risks[\"risks\"] if r[\"likelihood\"] == \"Medium\" and r[\"impact\"] == \"Medium\")\nlow = sum(1 for r in deployment_risks[\"risks\"] if r[\"likelihood\"] == \"Low\" or r[\"impact\"] == \"Low\")\n\ndeployment_risks[\"risk_matrix\"] = {\n    \"critical_risks\": critical,\n    \"high_risks\": high,\n    \"medium_risks\": medium,\n    \"low_risks\": low\n}\n\n# Save to JSON file\nwith open(\"deployment_risks.json\", \"w\") as f:\n    json.dump(deployment_risks, f, indent=2)\n\nprint(\"âœ… Deployment risks captured!\")\nprint(f\"\\nğŸš¨ Top 3 Risks Identified:\\n\")\nfor risk in deployment_risks[\"risks\"]:\n    print(f\"{risk['rank']}. {risk['title']}\")\n    print(f\"   ğŸ“ Category: {risk['category']}\")\n    print(f\"   ğŸ“Š Likelihood: {risk['likelihood']} | Impact: {risk['impact']}\")\n    print(f\"   ğŸ›¡ï¸  Current Mitigation: {risk['current_mitigation']}\")\n    print(f\"   ğŸ¯ M3 Mitigation: {risk['m3_mitigation']}\")\n    print()\n\nprint(f\"ğŸ“ˆ Risk Matrix:\")\nprint(f\"   ğŸ”´ Critical: {deployment_risks['risk_matrix']['critical_risks']}\")\nprint(f\"   ğŸŸ  High: {deployment_risks['risk_matrix']['high_risks']}\")\nprint(f\"   ğŸŸ¡ Medium: {deployment_risks['risk_matrix']['medium_risks']}\")\nprint(f\"   ğŸŸ¢ Low: {deployment_risks['risk_matrix']['low_risks']}\")\nprint(f\"\\nğŸ¯ Overall Readiness: {deployment_risks['overall_deployment_readiness']}\")\nprint(\"\\nğŸ’¡ Edit the risks above to match YOUR system's specific deployment risks, then re-run this cell.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n**SAVED_SECTION: 4** - Deployment risks captured to `deployment_risks.json`",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Section 5: Module 3 Deployment Goals\n\n**Purpose:** Define clear, measurable goals for what Module 3 will achieve.\n\n### The Deployment Vision:\n\n**Current State (Post-M2):**\n- Optimized, monitored, resilient RAG system\n- Running on localhost:8000\n- 25+ minutes manual deployment\n- 60+ minute MTTR\n- High deployment risks\n\n**Target State (Post-M3):**\n- Same system, but accessible to the world\n- Public HTTPS URL\n- One-command deployment\n- <5 minute MTTR with automated health checks\n- Deployment risks mitigated\n\n---\n\n### ğŸ¯ Core Module 3 Goals\n\nBased on the bridge document, Module 3 aims to solve these problems:\n\n#### 1. **One-Command Deployment** (M3.1 Containerization)\n- **Goal:** Package RAG system into Docker containers\n- **Success metric:** `docker-compose up` starts entire stack (API + Chroma + Redis + Prometheus + Grafana)\n- **Learning curve:** 4-6 hours (Docker fundamentals)\n- **Trade-off:** Adds deployment complexity, but provides portability\n\n#### 2. **Public HTTPS Access** (M3.2 Cloud Deployment)\n- **Goal:** Deploy to Railway or Render with automatic SSL\n- **Success metric:** Public URL like `https://your-rag.railway.app` accessible from anywhere\n- **Cost:** $5-50/month for cloud hosting\n- **Trade-off:** Monthly cost vs. 24/7 availability\n\n#### 3. **Automated Health Checks & Restarts** (M3.2 Cloud Deployment)\n- **Goal:** Cloud platform automatically restarts containers on failure\n- **Success metric:** MTTR reduces from 60+ minutes to <5 minutes\n- **Trade-off:** 10-20% performance overhead for health check endpoints\n\n#### 4. **Environment-Based Configuration** (M3.1 + M3.2)\n- **Goal:** Separate dev/prod environments with secrets management\n- **Success metric:** No hardcoded credentials, .env files not committed to Git\n- **Trade-off:** More configuration complexity\n\n#### 5. **Zero-Downtime Deployments** (M3.2 Cloud Deployment)\n- **Goal:** Update code without stopping service\n- **Success metric:** Rolling deployments with health checks\n- **Trade-off:** Requires blue-green deployment or canary releases (added complexity)\n\n#### 6. **Foundation for Horizontal Scaling** (M3.4 Load Testing)\n- **Goal:** Validate system under load, prepare for multi-instance scaling\n- **Success metric:** Handle 100-1000 requests/sec, identify bottlenecks\n- **Trade-off:** Scaling increases costs linearly\n\n---\n\n### ğŸ“Š Expected Improvements (Post-M3)\n\n**Deployment Time:**\n- Before: 25 minutes manual\n- After: <5 minutes automated (`git push` â†’ auto-deploy)\n- Improvement: 80% time reduction\n\n**Mean Time To Recovery (MTTR):**\n- Before: 60 minutes (manual detection + restart)\n- After: <5 minutes (automated health checks + restart)\n- Improvement: 90%+ MTTR reduction\n\n**Availability:**\n- Before: Limited to your laptop's uptime\n- After: 99%+ uptime with cloud hosting\n\n**Productivity:**\n- Before: 10-15 hours/month operational toil\n- After: 2-3 hours/month (monitoring + occasional fixes)\n- Improvement: 75% time savings\n\n**Cost Impact:**\n- New cost: $5-50/month cloud hosting\n- Time saved: 8-12 hours/month Ã— $50/hour = $400-600/month\n- Net benefit: $350-550/month\n\n---\n\n**SAVED_SECTION: 5** - Module 3 goals defined",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Section 6: Call-Forward - M3.1 Containerization Preview\n\n**Purpose:** Preview what you'll build in M3.1: Containerization with Docker\n\n---\n\n### ğŸ“¦ What is Containerization?\n\n**Docker** packages your application + all dependencies into a standardized unit (container) that runs identically anywhere.\n\n**Problem Docker solves:**\n- \"Works on my machine\" syndrome\n- Dependency conflicts between dev/prod environments\n- Complex multi-service deployments (API + Database + Cache + Monitoring)\n\n**How it works:**\n1. **Dockerfile** = Recipe for building your application container\n2. **docker-compose.yml** = Orchestration file defining all services (API, Chroma, Redis, Prometheus, Grafana)\n3. **docker-compose up** = One command to start entire stack\n\n---\n\n### ğŸ”§ M3.1 Preview: What You'll Build\n\n#### 1. **Dockerfile for FastAPI RAG Application**\n\nA Dockerfile that packages your RAG system:\n\n```dockerfile\n# Example preview - NOT final code\nFROM python:3.11-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Expose API port\nEXPOSE 8000\n\n# Health check endpoint\nHEALTHCHECK --interval=30s --timeout=3s --start-period=40s \\\n  CMD curl -f http://localhost:8000/health || exit 1\n\n# Start application\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n**What this does:**\n- Uses Python 3.11 base image\n- Installs your dependencies\n- Copies your code\n- Exposes port 8000\n- Defines health check endpoint\n- Starts FastAPI server\n\n---\n\n#### 2. **docker-compose.yml for Multi-Service Orchestration**\n\nA compose file orchestrating your entire stack:\n\n```yaml\n# Example preview - NOT final code\nversion: '3.8'\n\nservices:\n  # FastAPI RAG Application\n  api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - PINECONE_API_KEY=${PINECONE_API_KEY}\n    depends_on:\n      - redis\n      - chroma\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 3s\n      retries: 3\n\n  # Redis Cache\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis_data:/data\n\n  # ChromaDB Vector Database\n  chroma:\n    image: chromadb/chroma:latest\n    ports:\n      - \"8001:8000\"\n    volumes:\n      - chroma_data:/chroma/chroma\n\n  # Prometheus Monitoring\n  prometheus:\n    image: prom/prometheus:latest\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n      - prometheus_data:/prometheus\n\n  # Grafana Dashboards\n  grafana:\n    image: grafana/grafana:latest\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - grafana_data:/var/lib/grafana\n\nvolumes:\n  redis_data:\n  chroma_data:\n  prometheus_data:\n  grafana_data:\n```\n\n**What this does:**\n- Defines 5 services: API, Redis, Chroma, Prometheus, Grafana\n- Maps ports for each service\n- Manages environment variables\n- Sets up health checks\n- Creates persistent volumes for data\n\n---\n\n### ğŸš€ One-Command Deployment (Post-M3.1)\n\n**Before M3.1 (Current):**\n```bash\n# 12 manual steps\nredis-server &\nchroma run &\nprometheus --config.file=prometheus.yml &\ngrafana-server &\npython -m uvicorn main:app --reload\n# ... check logs, test endpoints, monitor ...\n```\n\n**After M3.1 (Goal):**\n```bash\n# 1 command\ndocker-compose up\n```\n\n**Result:**\n- All 5 services start automatically\n- Dependencies resolved in correct order\n- Health checks ensure services are ready\n- Logs aggregated in one terminal\n- Ctrl+C stops everything cleanly\n\n---\n\n### ğŸ“š M3.1 Learning Path\n\n**Duration:** 45-minute video + 2-3 hours hands-on practice\n\n**Topics covered:**\n1. **Docker fundamentals** (images, containers, volumes, networks)\n2. **Writing Dockerfile** for your FastAPI RAG application\n3. **Creating docker-compose.yml** for multi-service orchestration\n4. **Environment variable management** (.env files, secrets)\n5. **Health checks** and container restart policies\n6. **Volume mounting** for persistent data (Chroma vectors, Redis cache)\n7. **Testing locally** before cloud deployment\n\n**Learning curve:** 4-6 hours total (Docker is new for most learners)\n\n**Trade-offs acknowledged:**\n- Adds deployment complexity (need to learn Docker)\n- 10-20% performance overhead (containerization layer)\n- More configuration files to manage\n\n**Benefits:**\n- Portability: Runs identically on dev/prod\n- Consistency: \"Works on my machine\" eliminated\n- Foundation for cloud deployment in M3.2\n\n---\n\n### âœ… Readiness Check Before M3.1\n\nBefore starting M3.1, verify:\n\n1. **Module 2 complete:** Cost optimization, monitoring, error handling shipped\n2. **Code committed to Git:** All changes pushed, .env excluded\n3. **Docker installed:** `docker --version` and `docker-compose --version` work\n4. **Baseline captured:** This notebook completed with all JSON files saved\n5. **Goals clear:** You know what M3 will achieve (see Section 5)\n\n---\n\n### ğŸ¯ Next Steps\n\n1. **Complete this notebook:** Fill in all sections with YOUR actual data\n2. **Save baseline files:** Ensure `deployment_process.json`, `mttr_baseline.json`, `deployment_risks.json` are created\n3. **Commit baseline to Git:** Track your pre-M3 state for comparison\n4. **Install Docker:** If not already installed, get Docker Desktop for your OS\n5. **Start M3.1:** Containerization with Docker (45-minute video + 2-3 hours practice)\n\n---\n\n**SAVED_SECTION: 6** - Call-forward to M3.1 complete\n\n---\n\n## ğŸ‰ Baseline Capture Complete!\n\nYou've documented:\n- âœ… Module 2 achievements (recap)\n- âœ… Current manual deployment process (12 steps, 25 minutes)\n- âœ… MTTR baseline (60 minutes)\n- âœ… Top 3 deployment risks\n- âœ… Module 3 goals (one-command deploy, HTTPS, health checks)\n- âœ… M3.1 containerization preview (Dockerfile + docker-compose)\n\n**Files created:**\n- `deployment_process.json` - Current manual steps\n- `mttr_baseline.json` - Mean time to recovery metrics\n- `deployment_risks.json` - Top 3 deployment risks\n\n**Next:** Start M3.1 Containerization with Docker when ready!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}