{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# M1.1 → M1.2 Bridge — Readiness Checks & Call-Forward\n\n## Purpose\n\nYou mastered dense-only semantic search in M1.1 (embedding generation, Pinecone upserts, similarity thresholds). This bridge validates your setup is ready for **M1.2's hybrid search**, where dense vectors combine with sparse keyword matching to eliminate the 35–40% false-negative gap caused by terminology mismatches. Without confirming your index config, metadata structure, and baseline threshold now, M1.2's advanced techniques will fail silently or require costly re-indexing.\n\n## Concepts Covered\n\n- **Dense-only gap** (why semantic search alone misses exact-term matches)\n- **Readiness validation** (4 critical checks before hybrid indexing)\n- **Baseline documentation** (recording thresholds for M1.2 comparison)\n\n## After Completing\n\n- [ ] Verified Pinecone index is 1536-dimensional with cosine metric\n- [ ] Confirmed OpenAI embedding API access works without rate limits\n- [ ] Validated metadata contains original text fields (no re-index needed)\n- [ ] Documented baseline similarity threshold in `bridge_baseline.json`\n- [ ] Understood M1.2 preview: hybrid search, namespaces, alpha tuning, reranking\n\n## Context in Track\n\n**Bridge: M1.1 (Understanding Vector Databases) → M1.2 (Pinecone Data Model & Advanced Indexing)**\n\n---\n\n## Run Locally (Windows-first)\n\n```powershell\n# Windows PowerShell\n$env:PYTHONPATH=\"$PWD\"; jupyter notebook\n\n# macOS/Linux\nPYTHONPATH=$PWD jupyter notebook\n```\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Recap & Why Next (Dense-only gap → Hybrid)\n",
    "\n",
    "### What You Built in M1.1\n",
    "- ✅ **Production Pinecone Index** (1536-dim, serverless)\n",
    "- ✅ **Semantic Search Pipeline** (text → embedding → retrieval)\n",
    "- ✅ **Real Failure Debugging** (dimension mismatches, rate limits, metadata issues)\n",
    "- ✅ **Threshold Calibration** (domain-specific 0.6-0.9 tuning)\n",
    "\n",
    "### The Dense-Only Gap\n",
    "**Problem:** Dense vectors miss exact terminology matches.  \n",
    "**Example:** Query \"GPT-4 pricing tiers\" may miss docs using \"cost tiers\" instead.\n",
    "\n",
    "**Business Impact:**\n",
    "- 35-40% of documentation searches return irrelevant results\n",
    "- 20-30 minutes daily wasted per engineer\n",
    "- False-confident LLM responses with wrong context\n",
    "\n",
    "### The Trade-Off\n",
    "**Hybrid Search = Dense + Sparse vectors**\n",
    "- ⬆️ 20-40% better recall\n",
    "- ⬇️ 30-80ms added latency\n",
    "\n",
    "When dense-only is still superior:\n",
    "- Rapidly changing data\n",
    "- Latency-critical systems (<50ms)\n",
    "- Purely semantic queries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Section 2: Check 1 — Pinecone Index Config (dimension & metric)\n\nVerify your index is 1536-dimensional with cosine metric. Dimension mismatches cause all upserts to fail; wrong metrics break similarity ranking. The next cell connects to Pinecone and prints your index configuration.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\n\ntry:\n    from dotenv import load_dotenv\n    load_dotenv()\nexcept ImportError:\n    pass  # python-dotenv not installed; will use environment variables directly\n\nPINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\nPINECONE_INDEX = os.getenv(\"PINECONE_INDEX\", \"your-index\")\n\nif not PINECONE_API_KEY:\n    print(\"⚠️ Skipping (no keys): Set PINECONE_API_KEY in .env\")\nelse:\n    try:\n        from pinecone import Pinecone\n        pc = Pinecone(api_key=PINECONE_API_KEY)\n        index = pc.Index(PINECONE_INDEX)\n        stats = index.describe_index_stats()\n        print(f\"✅ Index: {PINECONE_INDEX}\")\n        print(f\"   Dimension: {stats.dimension}\")\n        # Expected: 1536, metric: cosine\n    except Exception as e:\n        print(f\"❌ Error: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 3: Check 2 — OpenAI Embedding Access\n\nTest embedding generation without rate limits. Hybrid search doubles embedding calls (dense + sparse), so rate limits block production. The next cell creates a test embedding and prints its dimension.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nif not OPENAI_API_KEY:\n    print(\"⚠️ Skipping (no keys): Set OPENAI_API_KEY in .env\")\nelse:\n    try:\n        from openai import OpenAI\n        client = OpenAI(api_key=OPENAI_API_KEY)\n        response = client.embeddings.create(\n            model=\"text-embedding-3-small\",\n            input=\"test query\"\n        )\n        vector = response.data[0].embedding\n        print(f\"✅ Embedding created: {len(vector)} dimensions\")\n        # Expected: 1536 dimensions\n    except Exception as e:\n        print(f\"❌ Error: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 4: Check 3 — Metadata Contains Original Text\n\nQuery one vector and inspect metadata for the original text field. Re-indexing 10K documents costs 20-30 minutes, so confirm your data is ready for hybrid search now. The next cell queries your index and prints metadata keys.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "DEFAULT_NAMESPACE = os.getenv(\"DEFAULT_NAMESPACE\", \"demo\")\n\nif not PINECONE_API_KEY:\n    print(\"⚠️ Skipping (no keys)\")\nelse:\n    try:\n        from pinecone import Pinecone\n        pc = Pinecone(api_key=PINECONE_API_KEY)\n        index = pc.Index(PINECONE_INDEX)\n        # Query with a test embedding\n        query_vector = [0.01] * 1536  # dummy vector\n        results = index.query(vector=query_vector, top_k=1, \n                             include_metadata=True, namespace=DEFAULT_NAMESPACE)\n        if results.matches:\n            meta = results.matches[0].metadata\n            print(f\"✅ Metadata keys: {list(meta.keys())[:3]}\")\n            # Expected: 'text' or similar field present\n        else:\n            print(\"⚠️ No vectors in index yet\")\n    except Exception as e:\n        print(f\"❌ Error: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 5: Check 4 — Baseline Dense Threshold (document your number)\n\nRecord your current similarity threshold for M1.2 comparison. This baseline lets you measure hybrid search improvements objectively. The next cell saves your threshold to `bridge_baseline.json`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\n\n# Record your baseline dense threshold here\nBASELINE_THRESHOLD = 0.75  # Adjust based on your M1.1 testing (0.6-0.9 range)\nRATIONALE = \"Balanced precision/recall for internal docs\"\n\nbaseline_data = {\n    \"threshold\": BASELINE_THRESHOLD,\n    \"rationale\": RATIONALE,\n    \"date_recorded\": \"2025-11-06\"\n}\n\nwith open(\"bridge_baseline.json\", \"w\") as f:\n    json.dump(baseline_data, f, indent=2)\n\nprint(f\"✅ Baseline recorded: {BASELINE_THRESHOLD}\")\nprint(f\"   Saved to bridge_baseline.json\")\n# Expected: File created with your threshold value",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 6: Next Up Preview — What You'll Build in M1.2\n\n### Five Advanced Capabilities Ahead\n\n**1. Hybrid Search (Dense + Sparse)**\n- 20-40% better recall for exact terminology matches\n- Trade-off: +30-80ms latency per query\n- Best for: documentation, legal text, technical specs\n\n**2. Advanced Namespaces**\n- Multi-tenant patterns supporting thousands of users\n- Data isolation without separate indexes\n- Cost-effective scaling strategy\n\n**3. Dynamic Alpha Tuning**\n- Automatically adjust dense/sparse balance per query type\n- Query-specific optimization (0.0 = sparse-only, 1.0 = dense-only)\n- Adaptive performance for mixed workloads\n\n**4. Reranking**\n- +15-25% quality improvement after initial retrieval\n- Trade-off: +50-100ms overhead\n- Cross-encoder models for final ranking\n\n**5. Performance Optimization**\n- 30-50% latency reduction through batching\n- Connection pooling best practices\n- Async patterns for high-throughput systems\n\n### When Dense-Only Remains Superior\n- Rapidly changing data (semantic drift)\n- Latency-critical systems (<50ms SLA)\n- Purely semantic queries (no exact-match needs)\n\n---\n\n**Ready to advance?** If all 4 checks passed, proceed to M1.2!  \n**Issues?** Review M1.1 setup before continuing.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}