{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M1.1 → M1.2 Bridge — Readiness Checks & Call-Forward\n",
    "\n",
    "**Purpose:** Validate your M1.1 setup before advancing to Hybrid Search & Advanced Indexing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Recap & Why Next (Dense-only gap → Hybrid)\n",
    "\n",
    "### What You Built in M1.1\n",
    "- ✅ **Production Pinecone Index** (1536-dim, serverless)\n",
    "- ✅ **Semantic Search Pipeline** (text → embedding → retrieval)\n",
    "- ✅ **Real Failure Debugging** (dimension mismatches, rate limits, metadata issues)\n",
    "- ✅ **Threshold Calibration** (domain-specific 0.6-0.9 tuning)\n",
    "\n",
    "### The Dense-Only Gap\n",
    "**Problem:** Dense vectors miss exact terminology matches.  \n",
    "**Example:** Query \"GPT-4 pricing tiers\" may miss docs using \"cost tiers\" instead.\n",
    "\n",
    "**Business Impact:**\n",
    "- 35-40% of documentation searches return irrelevant results\n",
    "- 20-30 minutes daily wasted per engineer\n",
    "- False-confident LLM responses with wrong context\n",
    "\n",
    "### The Trade-Off\n",
    "**Hybrid Search = Dense + Sparse vectors**\n",
    "- ⬆️ 20-40% better recall\n",
    "- ⬇️ 30-80ms added latency\n",
    "\n",
    "When dense-only is still superior:\n",
    "- Rapidly changing data\n",
    "- Latency-critical systems (<50ms)\n",
    "- Purely semantic queries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Section 2: Check 1 — Pinecone Index Config (dimension & metric)\n\n**Validation:** Confirm your index is 1536-dimensional with cosine metric.  \n**Why:** Dimension mismatches cause all upserts to fail; wrong metric breaks similarity ranking.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nPINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\nPINECONE_INDEX = os.getenv(\"PINECONE_INDEX\", \"your-index\")\n\nif not PINECONE_API_KEY:\n    print(\"⚠️ Skipping (no keys): Set PINECONE_API_KEY in .env\")\nelse:\n    try:\n        from pinecone import Pinecone\n        pc = Pinecone(api_key=PINECONE_API_KEY)\n        index = pc.Index(PINECONE_INDEX)\n        stats = index.describe_index_stats()\n        print(f\"✅ Index: {PINECONE_INDEX}\")\n        print(f\"   Dimension: {stats.dimension}\")\n        # Expected: 1536, metric: cosine\n    except Exception as e:\n        print(f\"❌ Error: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 3: Check 2 — OpenAI Embedding Access\n\n**Validation:** Test embedding generation without rate limits.  \n**Why:** Hybrid search doubles embedding calls (dense + sparse); rate limits block production.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nif not OPENAI_API_KEY:\n    print(\"⚠️ Skipping (no keys): Set OPENAI_API_KEY in .env\")\nelse:\n    try:\n        from openai import OpenAI\n        client = OpenAI(api_key=OPENAI_API_KEY)\n        response = client.embeddings.create(\n            model=\"text-embedding-3-small\",\n            input=\"test query\"\n        )\n        vector = response.data[0].embedding\n        print(f\"✅ Embedding created: {len(vector)} dimensions\")\n        # Expected: 1536 dimensions\n    except Exception as e:\n        print(f\"❌ Error: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 4: Check 3 — Metadata Contains Original Text\n\n**Validation:** Query 1 vector and inspect metadata for original text field.  \n**Why:** Re-indexing 10K documents costs 20-30 minutes; confirm data is ready for hybrid search.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "DEFAULT_NAMESPACE = os.getenv(\"DEFAULT_NAMESPACE\", \"demo\")\n\nif not PINECONE_API_KEY or not OPENAI_API_KEY:\n    print(\"⚠️ Skipping (no keys)\")\nelse:\n    try:\n        # Query with a test embedding\n        query_vector = [0.01] * 1536  # dummy vector\n        results = index.query(vector=query_vector, top_k=1, \n                             include_metadata=True, namespace=DEFAULT_NAMESPACE)\n        if results.matches:\n            meta = results.matches[0].metadata\n            print(f\"✅ Metadata keys: {list(meta.keys())[:3]}\")\n            # Expected: 'text' or similar field present\n        else:\n            print(\"⚠️ No vectors in index yet\")\n    except Exception as e:\n        print(f\"❌ Error: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 5: Check 4 — Baseline Dense Threshold (document your number)\n\n**Validation:** Record your current similarity threshold for M1.2 comparison.  \n**Why:** Measure M1.2 hybrid improvements against this baseline.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\n\n# Record your baseline dense threshold here\nBASELINE_THRESHOLD = 0.75  # Adjust based on your M1.1 testing (0.6-0.9 range)\nRATIONALE = \"Balanced precision/recall for internal docs\"\n\nbaseline_data = {\n    \"threshold\": BASELINE_THRESHOLD,\n    \"rationale\": RATIONALE,\n    \"date_recorded\": \"2025-11-06\"\n}\n\nwith open(\"bridge_baseline.json\", \"w\") as f:\n    json.dump(baseline_data, f, indent=2)\n\nprint(f\"✅ Baseline recorded: {BASELINE_THRESHOLD}\")\nprint(f\"   Saved to bridge_baseline.json\")\n# Expected: File created with your threshold value",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 6: Next Up Preview — What You'll Build in M1.2\n\n### Five Advanced Capabilities Ahead\n\n**1. Hybrid Search (Dense + Sparse)**\n- 20-40% better recall for exact terminology matches\n- Trade-off: +30-80ms latency per query\n- Best for: documentation, legal text, technical specs\n\n**2. Advanced Namespaces**\n- Multi-tenant patterns supporting thousands of users\n- Data isolation without separate indexes\n- Cost-effective scaling strategy\n\n**3. Dynamic Alpha Tuning**\n- Automatically adjust dense/sparse balance per query type\n- Query-specific optimization (0.0 = sparse-only, 1.0 = dense-only)\n- Adaptive performance for mixed workloads\n\n**4. Reranking**\n- +15-25% quality improvement after initial retrieval\n- Trade-off: +50-100ms overhead\n- Cross-encoder models for final ranking\n\n**5. Performance Optimization**\n- 30-50% latency reduction through batching\n- Connection pooling best practices\n- Async patterns for high-throughput systems\n\n### When Dense-Only Remains Superior\n- Rapidly changing data (semantic drift)\n- Latency-critical systems (<50ms SLA)\n- Purely semantic queries (no exact-match needs)\n\n---\n\n**Ready to advance?** If all 4 checks passed, proceed to M1.2!  \n**Issues?** Review M1.1 setup before continuing.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}