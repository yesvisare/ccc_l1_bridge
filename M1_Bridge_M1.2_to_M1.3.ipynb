{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge M1.2 → M1.3: Validation Notebook\n",
    "\n",
    "## Purpose\n",
    "\n",
    "You've just built production-ready hybrid search in M1.2 (sparse-dense vectors, namespaces, alpha tuning). This bridge validates those achievements and confirms your system is ready to accept automated document ingestion from M1.3. The shift: from manually preparing clean text chunks → building a pipeline that converts raw documents (PDF, DOCX, Markdown) into search-ready chunks in seconds instead of hours.\n",
    "\n",
    "---\n",
    "\n",
    "## Concepts Covered\n",
    "\n",
    "- **Validation checkpoints:** BM25 persistence, index metric configuration, namespace isolation, metadata size constraints\n",
    "- **Gap analysis:** Why manual document preparation doesn't scale (15-20 min/doc vs. 5-10 sec automated)\n",
    "- **Call-forward:** What M1.3's extract → clean → chunk → enrich pipeline solves\n",
    "\n",
    "---\n",
    "\n",
    "## After Completing\n",
    "\n",
    "- ✓ Verify your M1.2 hybrid search foundation meets production requirements\n",
    "- ✓ Identify any configuration gaps (wrong index metric, missing BM25 persistence, oversized metadata)\n",
    "- ✓ Understand why automated document processing is critical for scaling beyond manual data prep\n",
    "- ✓ Confirm readiness for M1.3 Document Processing Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## Context in Track\n",
    "\n",
    "**Bridge:** L1.M1.2 (Pinecone Hybrid Search) → L1.M1.3 (Document Processing Pipeline)  \n",
    "**Module:** Core RAG Architecture (M1)  \n",
    "**Type:** Within-module continuity connector\n",
    "\n",
    "---\n",
    "\n",
    "## Run Locally (Windows-First)\n",
    "\n",
    "```powershell\n",
    "# PowerShell\n",
    "$env:PYTHONPATH=\"$PWD\"; jupyter notebook\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Linux/Mac\n",
    "PYTHONPATH=$PWD jupyter notebook\n",
    "```\n",
    "\n",
    "**Optional:** Set API keys for full validation (notebook runs offline without them):\n",
    "```powershell\n",
    "$env:PINECONE_API_KEY=\"your-key\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Recap – Hybrid Search Achievements\n",
    "\n",
    "**What you accomplished in M1.2:**\n",
    "- ✓ **Hybrid search**: Sparse-dense combination (BM25 + OpenAI embeddings)\n",
    "- ✓ **Multi-tenant namespaces**: Isolated data per user\n",
    "- ✓ **Alpha parameter tuning**: Tested 0.2, 0.5, 0.8 for query balance\n",
    "- ✓ **Failure debugging**: Fixed all 5 common hybrid search issues\n",
    "\n",
    "**Impact:** 20-40% precision improvement over basic semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a summary of M1.2 achievements to confirm foundational hybrid search concepts are in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recap: Key hybrid search concepts\n",
    "print(\"M1.2 Achievements Validated:\")\n",
    "print(\"  ✓ Hybrid search (sparse + dense vectors)\")\n",
    "print(\"  ✓ Namespace isolation for multi-tenancy\")\n",
    "print(\"  ✓ Alpha tuning for semantic/keyword balance\")\n",
    "print(\"  ✓ Production-ready failure handling\")\n",
    "\n",
    "# Expected:\n",
    "# M1.2 Achievements Validated:\n",
    "#   ✓ Hybrid search (sparse + dense vectors)\n",
    "#   ✓ Namespace isolation for multi-tenancy\n",
    "#   ✓ Alpha tuning for semantic/keyword balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Readiness Check – BM25 Fitted & Saved\n",
    "\n",
    "**Requirement:** BM25 encoder must be fitted and serialized to disk.\n",
    "\n",
    "**Why it matters:**\n",
    "- Without saved encoder → 30-60s refit on every restart\n",
    "- Warning sign: \"BM25 encoder has not been fitted\" error\n",
    "\n",
    "**Validation:** Check for saved BM25 parameters file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for common BM25 serialization file names in the current directory. If absent, it's acceptable for now (M1.3 will cover document ingestion where BM25 fitting becomes critical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check for BM25 saved parameters\n",
    "bm25_files = [\"bm25_params.json\", \"bm25_encoder.pkl\", \"bm25.pkl\"]\n",
    "found = [f for f in bm25_files if Path(f).exists()]\n",
    "\n",
    "if found:\n",
    "    print(f\"✓ BM25 encoder saved: {found[0]}\")\n",
    "    print(f\"  Size: {Path(found[0]).stat().st_size} bytes\")\n",
    "else:\n",
    "    print(\"⚠️  No BM25 file found (stub ok - will be created in M1.3)\")\n",
    "    \n",
    "# Expected:\n",
    "# ✓ BM25 encoder saved: bm25_params.json\n",
    "#   Size: 1024 bytes\n",
    "# OR: ⚠️ No BM25 file found (stub ok - will be created in M1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Readiness Check – Index Metric is Dotproduct\n",
    "\n",
    "**Requirement:** Pinecone index must use `dotproduct` metric for hybrid search.\n",
    "\n",
    "**Why it matters:**\n",
    "- Wrong metric → hybrid search fails completely\n",
    "- Sparse values ONLY work with dotproduct metric\n",
    "- Warning sign: \"Sparse values are only supported with dotproduct metric\" error\n",
    "\n",
    "**Validation:** Query index configuration to verify metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to connect to Pinecone and check the index metric. If API keys are missing or Pinecone is not installed, skip gracefully (offline-friendly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offline-friendly: skip if no API key or pinecone not installed\n",
    "try:\n",
    "    from pinecone import Pinecone\n",
    "    import os\n",
    "    \n",
    "    api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"⚠️ Skipping (no keys) - Set PINECONE_API_KEY to validate\")\n",
    "    else:\n",
    "        pc = Pinecone(api_key=api_key)\n",
    "        # Assume first index or set your index name here\n",
    "        indexes = pc.list_indexes().names()\n",
    "        if indexes:\n",
    "            idx_name = indexes[0]\n",
    "            idx_info = pc.describe_index(idx_name)\n",
    "            metric = idx_info.metric\n",
    "            print(f\"✓ Index: {idx_name}\")\n",
    "            print(f\"  Metric: {metric}\")\n",
    "            if metric == \"dotproduct\":\n",
    "                print(\"  Status: ✓ Ready for hybrid search\")\n",
    "            else:\n",
    "                print(f\"  Status: ✗ Wrong metric (need dotproduct, got {metric})\")\n",
    "        else:\n",
    "            print(\"⚠️ No indexes found\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ Skipping (no keys) - pinecone not installed\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Skipping - Connection error: {type(e).__name__}\")\n",
    "    \n",
    "# Expected:\n",
    "# ✓ Index: hybrid-search-index\n",
    "#   Metric: dotproduct\n",
    "#   Status: ✓ Ready for hybrid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Readiness Check – Namespaces Isolation\n",
    "\n",
    "**Requirement:** Multi-tenant namespace isolation must prevent data leakage.\n",
    "\n",
    "**Why it matters:**\n",
    "- Without namespaces → all users see all data (security risk)\n",
    "- User-A's searches should NEVER return User-B's documents\n",
    "- Warning sign: Unexpected results from other tenants\n",
    "\n",
    "**Validation:** Sanity test namespace isolation logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually validate that namespace isolation strategy is understood (no external calls needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Namespace isolation sanity test (conceptual validation)\n",
    "test_namespaces = [\"user-123\", \"user-456\", \"tenant-a\", \"tenant-b\"]\n",
    "\n",
    "print(\"✓ Namespace isolation concept validated:\")\n",
    "print(\"  - Each user gets unique namespace ID\")\n",
    "print(\"  - Queries filter by namespace parameter\")\n",
    "print(\"  - Cross-namespace leakage prevented by design\")\n",
    "print(f\"  - Example namespaces: {', '.join(test_namespaces[:2])}\")\n",
    "\n",
    "# Expected:\n",
    "# ✓ Namespace isolation concept validated:\n",
    "#   - Each user gets unique namespace ID\n",
    "#   - Queries filter by namespace parameter\n",
    "#   - Cross-namespace leakage prevented by design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Readiness Check – Metadata Size < 40KB\n",
    "\n",
    "**Requirement:** Vector metadata must stay under 40KB per vector (Pinecone limit).\n",
    "\n",
    "**Why it matters:**\n",
    "- Oversized metadata → silent upsert failures (vectors lost)\n",
    "- Fewer search results than expected\n",
    "- Warning sign: Upsert succeeds but query returns incomplete results\n",
    "\n",
    "**Validation:** Test metadata size calculation logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate that metadata size calculation correctly identifies when a payload would exceed Pinecone's 40KB limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Test metadata size validation\n",
    "sample_metadata = {\n",
    "    \"source\": \"technical_doc.pdf\",\n",
    "    \"page\": 15,\n",
    "    \"chunk_id\": \"chunk-042\",\n",
    "    \"text_preview\": \"This is a sample preview text...\" * 10\n",
    "}\n",
    "\n",
    "metadata_size = sys.getsizeof(str(sample_metadata))\n",
    "limit_kb = 40000\n",
    "\n",
    "print(f\"✓ Metadata size validation:\")\n",
    "print(f\"  Sample metadata: {metadata_size} bytes\")\n",
    "print(f\"  Limit: {limit_kb} bytes (40KB)\")\n",
    "print(f\"  Status: {'✓ PASS' if metadata_size < limit_kb else '✗ FAIL'}\")\n",
    "\n",
    "# Expected:\n",
    "# ✓ Metadata size validation:\n",
    "#   Sample metadata: 512 bytes\n",
    "#   Limit: 40000 bytes (40KB)\n",
    "#   Status: ✓ PASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Call-Forward – Why Automated Document Processing Matters\n",
    "\n",
    "**The Problem:**\n",
    "Your hybrid search is production-ready... but feeding it clean data is still manual.\n",
    "\n",
    "**Manual document preparation doesn't scale:**\n",
    "- **Time cost:** 15-20 minutes per document to extract, clean, and chunk\n",
    "- **Scale limit:** 100 documents = 25-33 hours of manual work\n",
    "- **Error rate:** 10-15% inconsistency in chunk boundaries\n",
    "- **Maintenance burden:** Every new document type requires new extraction logic\n",
    "\n",
    "**What M1.3 Solves:**\n",
    "\n",
    "### 1. Automated Document Extraction\n",
    "Convert PDFs, Word docs, and Markdown to clean text with preserved structure.\n",
    "- **Trade-off:** Processing adds 5-10 seconds per document but eliminates 15-20 minutes of manual work\n",
    "- **Tools:** PyMuPDF for PDF extraction, format-specific handlers\n",
    "\n",
    "### 2. Intelligent Chunking Strategies\n",
    "Implement semantic chunking that respects sentence and paragraph boundaries.\n",
    "- **Trade-off:** Avoids quality degradation from mid-sentence splits\n",
    "- **Strategies:** Fixed-size, semantic, paragraph-aware chunking\n",
    "\n",
    "### 3. Metadata Enrichment Pipeline\n",
    "Extract and attach source attribution, page numbers, document type, and section headers.\n",
    "- **Benefit:** Better filtering and citation in search results\n",
    "- **Automation:** Consistent metadata across all processed documents\n",
    "\n",
    "**Bottom Line:**\n",
    "Process 100+ documents in **minutes** instead of **hours**, with consistent quality and full automation. M1.3 builds the ingestion system that feeds your M1.2 hybrid search engine.\n",
    "\n",
    "**Expected time:** 40 min video + 90-120 min hands-on practice\n",
    "\n",
    "---\n",
    "\n",
    "## Validation Complete\n",
    "\n",
    "If all readiness checks passed (or gracefully skipped), you're ready for M1.3!\n",
    "\n",
    "**Next:** M1.3 Document Processing Pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
