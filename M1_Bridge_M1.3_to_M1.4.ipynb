{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M1.3 ‚Üí M1.4 Bridge Validation\n",
    "\n",
    "## Purpose\n",
    "\n",
    "You've built a complete document processing pipeline in M1.3 that extracts, chunks, enriches, embeds, and stores documents in Pinecone. Before moving to M1.4's query pipeline, you must validate that your processed data meets minimum quality and completeness thresholds. Without this validation, query failures in M1.4 will be ambiguous‚Äîyou won't know if the problem is in retrieval logic or missing data. This notebook bridges the gap by confirming your vector database is query-ready.\n",
    "\n",
    "## Concepts Covered\n",
    "\n",
    "- **Vector database health checks:** Verifying minimum vector count and metadata completeness\n",
    "- **Pipeline documentation:** Ensuring chunking parameters are recorded for debugging\n",
    "- **Basic semantic search:** Testing retrieval with three query types (factual, how-to, comparison)\n",
    "- **Offline-friendly validation:** Graceful skipping when API keys are unavailable\n",
    "\n",
    "## After Completing\n",
    "\n",
    "You will be able to:\n",
    "- ‚úÖ Confirm your Pinecone index has ‚â•100 vectors with required metadata fields\n",
    "- ‚úÖ Validate that sample retrieval returns expected metadata keys (source, chunk_id, content_type)\n",
    "- ‚úÖ Document your chunking strategy for future debugging\n",
    "- ‚úÖ Run basic semantic queries to verify the pipeline works end-to-end\n",
    "- ‚úÖ Identify specific gaps that need remediation before M1.4\n",
    "\n",
    "## Context in Track\n",
    "\n",
    "**Bridge:** L1.M1.3 (Document Processing) ‚Üí L1.M1.4 (Query Pipeline)  \n",
    "**Track:** CCC RAG Fundamentals - Module 1  \n",
    "**Duration:** 15-20 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Locally (Windows-first)\n",
    "\n",
    "```powershell\n",
    "# Set environment variables\n",
    "$env:PINECONE_API_KEY=\"your-key-here\"\n",
    "$env:OPENAI_API_KEY=\"your-key-here\"\n",
    "$env:PINECONE_INDEX_NAME=\"rag-index\"\n",
    "\n",
    "# Launch notebook\n",
    "powershell -c \"$env:PYTHONPATH='$PWD'; jupyter notebook\"\n",
    "```\n",
    "\n",
    "**Linux/Mac:**\n",
    "```bash\n",
    "export PINECONE_API_KEY=\"your-key-here\"\n",
    "export OPENAI_API_KEY=\"your-key-here\"\n",
    "export PINECONE_INDEX_NAME=\"rag-index\"\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "**Note:** Notebook runs offline-friendly. Cells skip gracefully if API keys are missing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Recap - Pipeline Architecture\n",
    "\n",
    "**M1.3 Built:** Complete 6-stage document processing pipeline\n",
    "\n",
    "```\n",
    "extract ‚Üí clean ‚Üí chunk ‚Üí enrich ‚Üí embed ‚Üí store\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "- ‚úì Semantic chunking (15-20% overlap, sentence boundaries)\n",
    "- ‚úì Metadata enrichment (source, chunk_id, content_type)\n",
    "- ‚úì Embedding generation (OpenAI/sentence-transformers)\n",
    "- ‚úì Vector storage (Pinecone)\n",
    "\n",
    "**What's Next:** Query understanding, hybrid retrieval, reranking, response generation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Check - Pinecone Vector Count\n",
    "\n",
    "**Requirement:** ‚â•100 vectors with complete metadata\n",
    "\n",
    "**What this cell does:** Connects to Pinecone and queries index statistics to verify vector count meets the minimum threshold of 100 vectors. Skips gracefully if `PINECONE_API_KEY` is not set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Offline-friendly: Check for API key before attempting connection\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\", \"rag-index\")\n",
    "index = None\n",
    "\n",
    "if not api_key:\n",
    "    print(\"‚ö†Ô∏è  SKIP: PINECONE_API_KEY not set\")\n",
    "    print(\"   Set environment variable to run this check\")\n",
    "    print(\"   Example: export PINECONE_API_KEY='your-key-here'\")\n",
    "else:\n",
    "    try:\n",
    "        from pinecone import Pinecone\n",
    "        \n",
    "        pc = Pinecone(api_key=api_key)\n",
    "        index = pc.Index(index_name)\n",
    "        stats = index.describe_index_stats()\n",
    "        \n",
    "        vector_count = stats.get('total_vector_count', 0)\n",
    "        print(f\"üìä Vector Count: {vector_count}\")\n",
    "        \n",
    "        if vector_count >= 100:\n",
    "            print(f\"‚úÖ PASS: {vector_count} vectors (‚â•100 required)\")\n",
    "        else:\n",
    "            print(f\"‚ùå FAIL: {vector_count} vectors (need ‚â•100)\")\n",
    "            print(\"   ‚Üí Return to M1.3 to process more documents\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  ERROR: {str(e)}\")\n",
    "        print(\"   Cannot connect to Pinecone - check credentials and network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Check - Sample Retrieval & Metadata\n",
    "\n",
    "**Requirement:** Sample vectors have source, chunk_id, content_type metadata\n",
    "\n",
    "**What this cell does:** Queries Pinecone with a dummy vector to retrieve 3 sample records, then validates that all required metadata fields are present. This ensures M1.4's filtering and source attribution features will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offline-friendly: Skip if no API key or index unavailable\n",
    "if not api_key:\n",
    "    print(\"‚ö†Ô∏è  SKIP: No API key - cannot test retrieval\")\n",
    "elif index is None:\n",
    "    print(\"‚ö†Ô∏è  SKIP: Pinecone index not available from previous step\")\n",
    "else:\n",
    "    try:\n",
    "        # Create dummy query vector (zeros for sampling)\n",
    "        dummy_vector = [0.0] * 1536  # OpenAI embedding dimension\n",
    "        \n",
    "        # Query for 3 samples\n",
    "        results = index.query(vector=dummy_vector, top_k=3, include_metadata=True)\n",
    "        \n",
    "        if len(results['matches']) == 0:\n",
    "            print(\"‚ùå FAIL: No vectors returned from query\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Retrieved {len(results['matches'])} sample vectors\\n\")\n",
    "            \n",
    "            required_keys = ['source', 'chunk_id', 'content_type']\n",
    "            all_pass = True\n",
    "            \n",
    "            for i, match in enumerate(results['matches'][:3], 1):\n",
    "                metadata = match.get('metadata', {})\n",
    "                print(f\"Sample {i}: score={match['score']:.3f}\")\n",
    "                \n",
    "                missing = [k for k in required_keys if k not in metadata]\n",
    "                if missing:\n",
    "                    print(f\"   ‚ùå Missing keys: {missing}\")\n",
    "                    all_pass = False\n",
    "                else:\n",
    "                    print(f\"   ‚úÖ Has: {', '.join(required_keys)}\")\n",
    "                    # Show sample values (truncated)\n",
    "                    src = str(metadata.get('source', ''))[:40]\n",
    "                    print(f\"      source={src}...\")\n",
    "                print()\n",
    "            \n",
    "            if all_pass:\n",
    "                print(\"‚úÖ PASS: All samples have required metadata\")\n",
    "            else:\n",
    "                print(\"‚ùå FAIL: Some samples missing metadata keys\")\n",
    "                print(\"   ‚Üí Review M1.3 metadata enrichment step\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  ERROR: {str(e)}\")\n",
    "        print(\"   Cannot query index - check connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Check - Chunking Params Documentation\n",
    "\n",
    "**Requirement:** Chunking strategy documented (approach + parameters)\n",
    "\n",
    "**What this cell does:** Searches for existing chunking configuration files (config.json, chunking_config.json, README.md). If none exist, creates a default `chunking_params.json` with M1.3 pipeline settings to enable future debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Look for existing chunking config\n",
    "config_paths = [\n",
    "    \"config/chunking_config.json\",\n",
    "    \"chunking_config.json\",\n",
    "    \"config.json\",\n",
    "    \"README.md\"\n",
    "]\n",
    "\n",
    "found_config = None\n",
    "for path in config_paths:\n",
    "    if Path(path).exists():\n",
    "        found_config = path\n",
    "        break\n",
    "\n",
    "if found_config:\n",
    "    print(f\"‚úÖ Found config: {found_config}\")\n",
    "    if found_config.endswith('.json'):\n",
    "        with open(found_config) as f:\n",
    "            config = json.load(f)\n",
    "            print(f\"   Config keys: {list(config.keys())}\")\n",
    "    else:\n",
    "        print(f\"   (Documentation in {found_config})\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No config found - creating chunking_params.json\")\n",
    "    \n",
    "    # Default params based on M1.3 requirements\n",
    "    default_params = {\n",
    "        \"chunking_strategy\": \"semantic\",\n",
    "        \"chunk_size\": 512,\n",
    "        \"chunk_overlap\": 0.18,\n",
    "        \"overlap_description\": \"15-20% overlap\",\n",
    "        \"sentence_boundary\": True,\n",
    "        \"min_chunk_size\": 100,\n",
    "        \"embedding_model\": \"text-embedding-ada-002\",\n",
    "        \"embedding_dimension\": 1536,\n",
    "        \"notes\": \"Parameters for M1.3 document processing pipeline\"\n",
    "    }\n",
    "    \n",
    "    with open(\"chunking_params.json\", \"w\") as f:\n",
    "        json.dump(default_params, f, indent=2)\n",
    "    \n",
    "    print(\"‚úÖ Created chunking_params.json with default values\")\n",
    "    print(f\"   Strategy: {default_params['chunking_strategy']}\")\n",
    "    print(f\"   Chunk size: {default_params['chunk_size']}\")\n",
    "    print(f\"   Overlap: {default_params['overlap_description']}\")\n",
    "\n",
    "print(\"\\n‚úÖ PASS: Chunking strategy documented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Mini Smoke Test - Query Types\n",
    "\n",
    "**Requirement:** Test 3 query types (factual, how-to, comparison)\n",
    "\n",
    "**What this cell does:** Embeds three representative queries using OpenAI's API and retrieves the top matching chunk from Pinecone for each. Shows only the similarity score and source (truncated) to confirm end-to-end retrieval works before M1.4's advanced query pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offline-friendly: Skip if missing API keys\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"‚ö†Ô∏è  SKIP: PINECONE_API_KEY not set\")\n",
    "elif not openai_key:\n",
    "    print(\"‚ö†Ô∏è  SKIP: OPENAI_API_KEY not set\")\n",
    "    print(\"   Set environment variable to run smoke test\")\n",
    "    print(\"   Example: export OPENAI_API_KEY='your-key-here'\")\n",
    "elif index is None:\n",
    "    print(\"‚ö†Ô∏è  SKIP: Pinecone index not available\")\n",
    "else:\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=openai_key)\n",
    "        \n",
    "        # Test queries: factual, how-to, comparison\n",
    "        test_queries = [\n",
    "            (\"factual\", \"What is semantic chunking?\"),\n",
    "            (\"how-to\", \"How do I improve RAG accuracy?\"),\n",
    "            (\"comparison\", \"Compare dense vs sparse embeddings\")\n",
    "        ]\n",
    "        \n",
    "        print(\"üß™ Smoke Test: 3 Query Types\\n\")\n",
    "        \n",
    "        for query_type, query in test_queries:\n",
    "            # Embed query\n",
    "            response = client.embeddings.create(\n",
    "                input=query,\n",
    "                model=\"text-embedding-ada-002\"\n",
    "            )\n",
    "            query_vec = response.data[0].embedding\n",
    "            \n",
    "            # Query Pinecone (top 1 only)\n",
    "            results = index.query(vector=query_vec, top_k=1, include_metadata=True)\n",
    "            \n",
    "            if results['matches']:\n",
    "                top = results['matches'][0]\n",
    "                score = top['score']\n",
    "                metadata = top.get('metadata', {})\n",
    "                source = metadata.get('source', 'N/A')[:30]\n",
    "                \n",
    "                print(f\"{query_type.upper()}: \\\"{query}\\\"\")\n",
    "                print(f\"  ‚Üí score={score:.3f}, source={source}...\")\n",
    "            else:\n",
    "                print(f\"{query_type.upper()}: \\\"{query}\\\"\")\n",
    "                print(f\"  ‚Üí No results\")\n",
    "            print()\n",
    "        \n",
    "        print(\"‚úÖ PASS: Query pipeline functional\")\n",
    "        print(\"   (M1.4 will add: query understanding, rerank, citations)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  ERROR: {str(e)}\")\n",
    "        print(\"   Cannot complete smoke test - check API keys and network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Call-Forward - M1.4 Query Pipeline\n",
    "\n",
    "**Next Module:** M1.4 Query Pipeline & Response Generation\n",
    "\n",
    "**What You'll Build:**\n",
    "\n",
    "### 1. Query Understanding\n",
    "- Query classification (factual/how-to/comparison)\n",
    "- Query expansion (synonyms, related terms)\n",
    "- Query preprocessing pipeline\n",
    "\n",
    "### 2. Hybrid Retrieval\n",
    "- Semantic search (dense vectors)\n",
    "- Keyword search (BM25/sparse vectors)\n",
    "- Fusion strategies for 20-40% better recall\n",
    "\n",
    "### 3. Reranking & Citations\n",
    "- Cross-encoder scoring for relevance\n",
    "- Response generation with GPT-4\n",
    "- Source attribution with proper citations\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- Complete query-to-answer pipeline\n",
    "- Intelligent question answering system\n",
    "- Production-grade RAG capabilities\n",
    "\n",
    "**Trade-offs:**\n",
    "- Adds 200-400ms latency per query\n",
    "- 5 new pipeline components (classifier ‚Üí expander ‚Üí retriever ‚Üí reranker ‚Üí generator)\n",
    "- $150-500/month infrastructure costs\n",
    "\n",
    "**Duration:** ~44 min video + 60 min hands-on\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for M1.4!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
