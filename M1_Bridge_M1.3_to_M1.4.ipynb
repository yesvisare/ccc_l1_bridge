{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M1.3 ‚Üí M1.4 Bridge Validation Notebook\n",
    "\n",
    "**Purpose:** Validate document processing pipeline before moving to query pipeline\n",
    "\n",
    "**From:** M1.3 Document Processing Pipeline  \n",
    "**To:** M1.4 Query Pipeline & Response Generation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Recap - Pipeline Architecture\n",
    "\n",
    "**M1.3 Built:** Complete 6-stage document processing pipeline\n",
    "\n",
    "```\n",
    "extract ‚Üí clean ‚Üí chunk ‚Üí enrich ‚Üí embed ‚Üí store\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "- ‚úì Semantic chunking (15-20% overlap, sentence boundaries)\n",
    "- ‚úì Metadata enrichment (source, chunk_id, content_type)\n",
    "- ‚úì Embedding generation (OpenAI/sentence-transformers)\n",
    "- ‚úì Vector storage (Pinecone)\n",
    "\n",
    "**What's Next:** Query understanding, hybrid retrieval, reranking, response generation\n",
    "\n",
    "---\n",
    "\n",
    "**SAVED_SECTION: 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Section 2: Check - Pinecone Vector Count\n\n**Requirement:** ‚â•100 vectors with complete metadata\n\n**Test:** Query Pinecone stats API",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nfrom pinecone import Pinecone\n\n# Check 2.1: Pinecone connection and stats\ntry:\n    api_key = os.getenv(\"PINECONE_API_KEY\")\n    if not api_key:\n        print(\"‚ùå FAIL: PINECONE_API_KEY not set\")\n        print(\"   Skip gracefully - no API key available\")\n    else:\n        pc = Pinecone(api_key=api_key)\n        index_name = os.getenv(\"PINECONE_INDEX_NAME\", \"rag-index\")\n        \n        # Expected: index stats showing vector count\n        index = pc.Index(index_name)\n        stats = index.describe_index_stats()\n        \n        vector_count = stats.get('total_vector_count', 0)\n        print(f\"üìä Vector Count: {vector_count}\")\n        \n        if vector_count >= 100:\n            print(f\"‚úÖ PASS: {vector_count} vectors (‚â•100 required)\")\n        else:\n            print(f\"‚ùå FAIL: {vector_count} vectors (need ‚â•100)\")\n            print(\"   ‚Üí Return to M1.3 to process more documents\")\n            \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  ERROR: {str(e)}\")\n    print(\"   Skip gracefully - cannot connect to Pinecone\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n**SAVED_SECTION: 2**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Section 3: Check - Sample Retrieval & Metadata\n\n**Requirement:** Sample vectors have source, chunk_id, content_type metadata\n\n**Test:** Fetch 3 sample vectors and validate metadata keys",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check 3.1: Sample retrieval with metadata validation\ntry:\n    if not api_key:\n        print(\"‚ö†Ô∏è  SKIP: No API key - cannot test retrieval\")\n    else:\n        # Create dummy query vector (zeros for sampling)\n        import numpy as np\n        dummy_vector = [0.0] * 1536  # OpenAI embedding dimension\n        \n        # Query for 3 samples\n        results = index.query(vector=dummy_vector, top_k=3, include_metadata=True)\n        \n        if len(results['matches']) == 0:\n            print(\"‚ùå FAIL: No vectors returned from query\")\n        else:\n            print(f\"‚úÖ Retrieved {len(results['matches'])} sample vectors\\n\")\n            \n            required_keys = ['source', 'chunk_id', 'content_type']\n            all_pass = True\n            \n            for i, match in enumerate(results['matches'][:3], 1):\n                metadata = match.get('metadata', {})\n                print(f\"Sample {i}: score={match['score']:.3f}\")\n                \n                missing = [k for k in required_keys if k not in metadata]\n                if missing:\n                    print(f\"   ‚ùå Missing keys: {missing}\")\n                    all_pass = False\n                else:\n                    print(f\"   ‚úÖ Has: {', '.join(required_keys)}\")\n                    # Show sample values (truncated)\n                    src = str(metadata.get('source', ''))[:40]\n                    print(f\"      source={src}...\")\n                print()\n            \n            if all_pass:\n                print(\"‚úÖ PASS: All samples have required metadata\")\n            else:\n                print(\"‚ùå FAIL: Some samples missing metadata keys\")\n                print(\"   ‚Üí Review M1.3 metadata enrichment step\")\n                \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  ERROR: {str(e)}\")\n    print(\"   Skip gracefully\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n**SAVED_SECTION: 3**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Section 4: Check - Chunking Params Documentation\n\n**Requirement:** Chunking strategy documented (approach + parameters)\n\n**Test:** Check for config file or create JSON record if missing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nfrom pathlib import Path\n\n# Check 4.1: Look for existing chunking config\nconfig_paths = [\n    \"config/chunking_config.json\",\n    \"chunking_config.json\",\n    \"config.json\",\n    \"README.md\"\n]\n\nfound_config = None\nfor path in config_paths:\n    if Path(path).exists():\n        found_config = path\n        break\n\nif found_config:\n    print(f\"‚úÖ Found config: {found_config}\")\n    if found_config.endswith('.json'):\n        with open(found_config) as f:\n            config = json.load(f)\n            print(f\"   Config keys: {list(config.keys())}\")\n    else:\n        print(f\"   (Documentation in {found_config})\")\nelse:\n    print(\"‚ö†Ô∏è  No config found - creating chunking_params.json\")\n    \n    # Default params based on M1.3 requirements\n    default_params = {\n        \"chunking_strategy\": \"semantic\",\n        \"chunk_size\": 512,\n        \"chunk_overlap\": 0.18,\n        \"overlap_description\": \"15-20% overlap\",\n        \"sentence_boundary\": True,\n        \"min_chunk_size\": 100,\n        \"embedding_model\": \"text-embedding-ada-002\",\n        \"embedding_dimension\": 1536,\n        \"notes\": \"Parameters for M1.3 document processing pipeline\"\n    }\n    \n    with open(\"chunking_params.json\", \"w\") as f:\n        json.dump(default_params, f, indent=2)\n    \n    print(\"‚úÖ Created chunking_params.json with default values\")\n    print(f\"   Strategy: {default_params['chunking_strategy']}\")\n    print(f\"   Chunk size: {default_params['chunk_size']}\")\n    print(f\"   Overlap: {default_params['overlap_description']}\")\n\nprint(\"\\n‚úÖ PASS: Chunking strategy documented\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n**SAVED_SECTION: 4**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Section 5: Mini Smoke Test - Query Types\n\n**Requirement:** Test 3 query types (factual, how-to, comparison)\n\n**Test:** Basic semantic search ‚Üí show top score only",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check 5.1: Mini smoke test with 3 query types\ntry:\n    openai_key = os.getenv(\"OPENAI_API_KEY\")\n    \n    if not api_key or not openai_key:\n        print(\"‚ö†Ô∏è  SKIP: Missing API keys\")\n    else:\n        from openai import OpenAI\n        client = OpenAI(api_key=openai_key)\n        \n        # Test queries: factual, how-to, comparison\n        test_queries = [\n            (\"factual\", \"What is semantic chunking?\"),\n            (\"how-to\", \"How do I improve RAG accuracy?\"),\n            (\"comparison\", \"Compare dense vs sparse embeddings\")\n        ]\n        \n        print(\"üß™ Smoke Test: 3 Query Types\\n\")\n        \n        for query_type, query in test_queries:\n            # Embed query\n            response = client.embeddings.create(\n                input=query,\n                model=\"text-embedding-ada-002\"\n            )\n            query_vec = response.data[0].embedding\n            \n            # Query Pinecone (top 1 only)\n            results = index.query(vector=query_vec, top_k=1, include_metadata=True)\n            \n            if results['matches']:\n                top = results['matches'][0]\n                score = top['score']\n                metadata = top.get('metadata', {})\n                source = metadata.get('source', 'N/A')[:30]\n                \n                print(f\"{query_type.upper()}: \\\"{query}\\\"\")\n                print(f\"  ‚Üí score={score:.3f}, source={source}...\")\n            else:\n                print(f\"{query_type.upper()}: \\\"{query}\\\"\")\n                print(f\"  ‚Üí No results\")\n            print()\n        \n        print(\"‚úÖ PASS: Query pipeline functional\")\n        print(\"   (M1.4 will add: query understanding, rerank, citations)\")\n        \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  ERROR: {str(e)}\")\n    print(\"   Skip gracefully\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n**SAVED_SECTION: 5**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Section 6: Call-Forward - M1.4 Query Pipeline\n\n**Next Module:** M1.4 Query Pipeline & Response Generation\n\n**What You'll Build:**\n\n### 1. Query Understanding\n- Query classification (factual/how-to/comparison)\n- Query expansion (synonyms, related terms)\n- Query preprocessing pipeline\n\n### 2. Hybrid Retrieval\n- Semantic search (dense vectors)\n- Keyword search (BM25/sparse vectors)\n- Fusion strategies for 20-40% better recall\n\n### 3. Reranking & Citations\n- Cross-encoder scoring for relevance\n- Response generation with GPT-4\n- Source attribution with proper citations\n\n**Expected Outcomes:**\n- Complete query-to-answer pipeline\n- Intelligent question answering system\n- Production-grade RAG capabilities\n\n**Trade-offs:**\n- Adds 200-400ms latency per query\n- 5 new pipeline components (classifier ‚Üí expander ‚Üí retriever ‚Üí reranker ‚Üí generator)\n- $150-500/month infrastructure costs\n\n**Duration:** ~44 min video + 60 min hands-on\n\n---\n\n**Ready for M1.4!**\n\n---\n\n**SAVED_SECTION: 6**",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}